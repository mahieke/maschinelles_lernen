{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Testklassifikation mit Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "#for float division\n",
    "from __future__ import division\n",
    "# import the time model to allow python to pause.\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from numpy import linalg as LA\n",
    "import scipy as sp\n",
    "import urllib2\n",
    "from urllib2 import urlopen, URLError, HTTPError\n",
    "import tarfile as tar\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "downloadFileName = '20news-18828.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get folder path containing text files\n",
    "path = os.path.curdir+'/'+downloadFileName\n",
    "folder_name = \"20news-18828/\"\n",
    "\n",
    "subFolder = ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n",
    "\n",
    "tar_file = tar.open(path, mode='r:gz')\n",
    "\n",
    "for dir_name in subFolder:\n",
    "    subdir_and_files = [\n",
    "        tarinfo for tarinfo in tar_file.getmembers()\n",
    "        if tarinfo.name.startswith(folder_name + dir_name)\n",
    "        ]\n",
    "    tar_file.extractall(members=subdir_and_files)\n",
    "    \n",
    "tar_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Lesen Sie alle Dateien aus diesen vier Verzeichnissen in eine Array von Strings ein (d.h. ein File in einem String). Speichern Sie zusätzlich die Klassenzugehörigkeit jedes Dokuments in einem Vektor ab (Kontrolle: Sie müssten jetzt 3387 Strings im Speicher haben)</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3387\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "labels = []\n",
    "\n",
    "for dir_name in subFolder:\n",
    "    \n",
    "    directory_path = folder_name + dir_name\n",
    "    file_names = os.listdir(directory_path)\n",
    "    for name in file_names:\n",
    "        \n",
    "        f = open(directory_path + '/' + name, 'r')\n",
    "        data.append(f.read())\n",
    "        f.close()\n",
    "        labels.append(dir_name)\n",
    "        \n",
    "data_length = len(data)\n",
    "print data_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Im nächsten Schritt muss jeder String in Worte (Tokens) zerlegt werden, die durch Leerzeichen, Kommas etc. voneinander getrennt sind. Hierzu setzen wir das Python-Standardpaket re ein, das zur Analyse regulärer Ausdrücke dient. Durch folgenden Befehl werden alle Tokens eines Strings textline in einer Liste l gespeichert, nachdem er zuvor mit\n",
    "lower() in Kleinbuchstaben umgewandelt wurde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def strip_header ( text ):\n",
    "    _before,_blankline,after = text.partition ('\\n\\n')\n",
    "    return after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenStringToList(data):\n",
    "    l = re.compile(r\"(?u)\\b\\w\\w+\\b\").findall(data.lower())\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41777\n"
     ]
    }
   ],
   "source": [
    "token_list = []\n",
    "\n",
    "for i in range(0, len(data)):\n",
    "    l = tokenStringToList(strip_header (data[i]))\n",
    "    \n",
    "    for j in range(0, len(l)):\n",
    "        \n",
    "        if l[j] not in token_list:\n",
    "            token_list.append(l[j])\n",
    "            \n",
    "vector_length = len(token_list)\n",
    "\n",
    "print vector_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Berechnen Sie für jeden Text einen\n",
    "Merkmalsvektor, der für jedes Wort des Vokabulars seine Häuﬁgkeit innerhalb des Texts\n",
    "enthält."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matrix = np.zeros((data_length, vector_length))\n",
    "\n",
    "for i in range(0, len(data)):\n",
    "    j = 0\n",
    "    for token in token_list:\n",
    "        matrix[i][j] = data[i].count(token)\n",
    "        j += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3387"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Verwenden Sie die ersten 60% der Daten als Trainingsdatensatz, die restlichen als Testdatensatz. Trainieren Sie damit einen multinomialen naiven Bayes-Klassiﬁkator. Bestimmen\n",
    "Sie den Anteil korrekter Klassiﬁkationen auf Ihren Trainings- und Testdaten. Wie gut\n",
    "generalisiert Ihr Klassiﬁkator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classification(vector, labels, p_class, p_word_in_class):\n",
    "    p = np.zeros((len(p_class)))\n",
    "    for i in range (0, len(p_class)):\n",
    "        p[i] = p_class[i] + np.sum(np.multiply(p_word_in_class[i], vector))\n",
    "    c = np.argmax(p)\n",
    "    return labels[c]\n",
    "\n",
    "def testClassification(test_data, test_label, log_p_class, log_p_word_in_class):\n",
    "    hit = 0.0\n",
    "\n",
    "    for i in range (0, len(test_data)):\n",
    "        test_index = i\n",
    "        class_name = classification(test_data[test_index], subFolder, log_p_class, log_p_word_in_class)\n",
    "        if test_label[i] == class_name:\n",
    "            hit += 1\n",
    "\n",
    "    hit /= len(test_data)\n",
    "    hit *= 100\n",
    "\n",
    "    s = \"Korrekte Klassifikationen: %.2f%%\"% (hit)\n",
    "    print s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_files = []\n",
    "\n",
    "for dir_name in subFolder:\n",
    "    directory_path = folder_name + dir_name\n",
    "    count_files.append(len(os.listdir(directory_path)))\n",
    "    \n",
    "count_training = 0 # 60%\n",
    "count_test = 0 # 40%\n",
    "\n",
    "for i in range (0, len(count_files)):     \n",
    "    count_60 = (int)(round(0.6 * count_files[i], 0))\n",
    "    count_training += count_60\n",
    "    count_test += (count_files[i] - count_60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data = np.zeros((count_training,vector_length))\n",
    "test_data = np.zeros((count_test,vector_length))\n",
    "training_label = []\n",
    "test_label = []\n",
    "\n",
    "index_start = 0\n",
    "index_end = 0\n",
    "index_training = 0\n",
    "index_test = 0\n",
    "\n",
    "for dir_name in subFolder:\n",
    "    \n",
    "    directory_path = folder_name + dir_name\n",
    "    count_files = len(os.listdir(directory_path))\n",
    "    count_60 = (int)(round(0.6 * count_files, 0))\n",
    "    index_end += count_files\n",
    "    \n",
    "    for i in range(index_start, index_end):\n",
    "        if i < index_start + count_60:\n",
    "            training_data[index_training,:] = matrix[i,:]\n",
    "            index_training += 1\n",
    "            training_label.append(dir_name)\n",
    "        else:\n",
    "            test_data[index_test,:] = matrix[i,:]\n",
    "            index_test += 1\n",
    "            test_label.append(dir_name)\n",
    "    \n",
    "    index_start = index_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_class = np.zeros((len(subFolder))) \n",
    "num_words_class = np.zeros((len(subFolder)))   \n",
    "num_words_in_vocabulary = len(training_data[0])\n",
    "p_num_words_in_class = np.zeros((len(subFolder), len(training_data[0])))\n",
    "\n",
    "for i in range(0, len(training_data)):\n",
    "    label = training_label[i]\n",
    "    vector = training_data[i]\n",
    "\n",
    "    if label == subFolder[0]:\n",
    "        class_index = 0\n",
    "    elif label == subFolder[1]:\n",
    "        class_index = 1\n",
    "    elif label == subFolder[2]:\n",
    "        class_index = 2\n",
    "    elif label == subFolder[3]:\n",
    "        class_index = 3\n",
    "        \n",
    "    p_class[class_index] += 1\n",
    "    num_words_class[class_index] += np.sum(vector)\n",
    "    p_num_words_in_class[class_index] += vector\n",
    "\n",
    "p_class /= len(training_data)\n",
    "\n",
    "for i in range(0, len(p_num_words_in_class)):\n",
    "    p_num_words_in_class[i] += 1\n",
    "    p_num_words_in_class[i] = p_num_words_in_class[i] / (num_words_class[i] + num_words_in_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_p_class = np.log10(p_class)\n",
    "log_p_word_in_class = np.log10(p_num_words_in_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Korrekte Klassifikationen: 91.73%\n"
     ]
    }
   ],
   "source": [
    "#Testdaten\n",
    "testClassification(test_data, test_label, log_p_class, log_p_word_in_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Korrekte Klassifikationen: 94.78%\n"
     ]
    }
   ],
   "source": [
    "#Trainingsdaten\n",
    "testClassification(training_data, training_label, log_p_class, log_p_word_in_class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
